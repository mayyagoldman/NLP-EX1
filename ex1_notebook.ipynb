{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "PROB = 0\n",
    "WORD = 1\n",
    "UNIGRAM = 1\n",
    "BIGRAM = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.nlp = None\n",
    "        self.unigram_data = {}  # {w:count}\n",
    "        self.bigram_data = {}  # {w_prev: {w:count}}\n",
    "        self.M = 0\n",
    "        self.m = 0\n",
    "        self.unigram_data_size = 0\n",
    "        self.bigram_data_size = {}\n",
    "\n",
    "    def add_to_unigram(self, w):\n",
    "        self.unigram_data_size += 1\n",
    "        if w in self.unigram_data:\n",
    "            self.unigram_data[w] += 1\n",
    "        else:\n",
    "            self.unigram_data[w] = 1\n",
    "\n",
    "    def add_to_bigram(self, w, w_prev):\n",
    "        if w_prev not in self.bigram_data:\n",
    "            self.bigram_data[w_prev] = {}\n",
    "            self.bigram_data_size[w_prev] = 0\n",
    "        self.bigram_data_size[w_prev] += 1\n",
    "        w_prev_dict = self.bigram_data[w_prev]\n",
    "        if w in w_prev_dict:\n",
    "            w_prev_dict[w] += 1\n",
    "        else:\n",
    "            w_prev_dict[w] = 1\n",
    "\n",
    "    def load_data(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n",
    "        for text in dataset['text']:\n",
    "            self.m +=1\n",
    "            doc = self.nlp(text)\n",
    "            w_prev = 'START'\n",
    "            for w in doc:\n",
    "                if w.is_alpha:\n",
    "                    self.M +=1\n",
    "                    self.add_to_unigram(w.lemma_)\n",
    "                    self.add_to_bigram(w.lemma_, w_prev)\n",
    "                    w_prev = w.lemma_  # todo: check first word in extreme cases is indeed START\n",
    "\n",
    "        with open('unigram_data.txt', 'w') as f:\n",
    "            json.dump(self.unigram_data, f, ensure_ascii=False)\n",
    "        with open('bigram_data.txt', 'w') as f:\n",
    "            json.dump(self.bigram_data, f, ensure_ascii=False)\n",
    "        with open('metadata.txt', 'w') as f:\n",
    "            json.dump({'M': self.M , 'm':self.m , 'unigram_data_size': self.unigram_data_size  ,\n",
    "                       'bigram_data_size' :self.bigram_data_size}, f, ensure_ascii=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "class Unigram:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        with open('unigram_data.txt') as json_file:\n",
    "            self.corpus = json.load(json_file)\n",
    "        with open('metadata.txt') as json_file:\n",
    "            meta = json.load(json_file)\n",
    "        self.M = meta['M'] #total num of tokens\n",
    "        self.m = meta['m'] #total num of sentences\n",
    "        self.max_prob = - np.inf\n",
    "        self.max_prob_word = None\n",
    "        self.corpus_size = meta['unigram_data_size']\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for w in self.corpus.keys():\n",
    "            count = self.corpus[w]\n",
    "            prob_w = np.log(count / self.corpus_size)\n",
    "            self.corpus[w] = prob_w\n",
    "            if prob_w > self.max_prob:\n",
    "                self.max_prob = prob_w\n",
    "                self.max_prob_word = w\n",
    "\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        return self.max_prob_word\n",
    "\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        prob = 0\n",
    "        for w in sentence:\n",
    "            if w.lemma_ not in self.corpus:\n",
    "                return - np.inf\n",
    "            prob += self.corpus[w.lemma_]\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def perplexity(self ,test_set):\n",
    "        M = 0\n",
    "        prob_sum = 0\n",
    "        for sentence in test_set:\n",
    "            M += len(sentence)\n",
    "            prob_sum += self.probability(sentence)\n",
    "        l = prob_sum / M\n",
    "        return np.exp(-l)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "class Bigram:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        with open('bigram_data.txt') as json_file:\n",
    "            self.corpus = json.load(json_file)\n",
    "        with open('metadata.txt') as json_file:\n",
    "            meta = json.load(json_file)\n",
    "        self.M = meta['M'] #total num of tokens\n",
    "        self.m = meta['m'] #total num of sentences\n",
    "        self.max_probs = {}\n",
    "        self.pair_counts = meta['bigram_data_size']\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for first_w  in self.corpus.keys():\n",
    "            next_w = self.corpus[first_w]\n",
    "            self.max_probs[first_w] = [- np.inf, None]\n",
    "            for w in next_w.keys():\n",
    "                count = next_w[w]\n",
    "                pair_prob = np.log(count / self.pair_counts[first_w])\n",
    "                next_w[w] = pair_prob\n",
    "                if pair_prob > self.max_probs[first_w][0]:\n",
    "                    self.max_probs[first_w][PROB] = pair_prob\n",
    "                    self.max_probs[first_w][WORD] = w\n",
    "\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        last_word = sentence[-1]\n",
    "        last_word = last_word.lemma_\n",
    "        if last_word not in self.corpus:\n",
    "            return \"STOP\"\n",
    "        return self.max_probs[last_word][WORD]\n",
    "\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        if sentence[0].lemma_ not in self.corpus['START']:\n",
    "            return - np.inf\n",
    "        prob = self.corpus['START'][sentence[0].lemma_]\n",
    "        for i in range(2, len(sentence)):\n",
    "            w_prev = sentence[i - 1].lemma_\n",
    "            w_curr = sentence[i].lemma_\n",
    "            if w_prev not in self.corpus:\n",
    "                return - np.inf\n",
    "            if w_curr not in self.corpus[w_prev]:\n",
    "                return - np.inf\n",
    "            prob += self.corpus[w_prev][w_curr]\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def perplexity(self , test_set):\n",
    "        M = 0\n",
    "        prob_sum = 0\n",
    "        for sentence in test_set:\n",
    "            M += len(sentence)\n",
    "            prob_sum += self.probability(sentence)\n",
    "        l = prob_sum / M\n",
    "        return np.exp(-l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "class LinearInterpolation:\n",
    "    def __init__(self, unigram, bigram , l1 , l2):\n",
    "        self.unigram_model = unigram\n",
    "        self.bigram_model = bigram\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        unigram_prob = self.unigram_model.probability(sentence)\n",
    "        if unigram_prob == -np.inf:\n",
    "            unigram_prob = 0\n",
    "        bigram_prob = self.bigram_model.probability(sentence)\n",
    "        if bigram_prob == -np.inf:\n",
    "            bigram_prob = 0\n",
    "        return (self.l1 * unigram_prob) + (self.l2 * bigram_prob)\n",
    "\n",
    "    def perplexity(self , test_set):\n",
    "        M = 0\n",
    "        prob_sum = 0\n",
    "        for sentence in test_set:\n",
    "            M += len(sentence)\n",
    "            prob_sum += self.probability(sentence)\n",
    "        l = prob_sum / M\n",
    "        return np.exp(-l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "# corpus = Corpus()\n",
    "# corpus.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "\"\"\"1. Train maximum-likelihood unigram and bigram language models based on the above training data.\"\"\"\n",
    "\n",
    "unigram_model = Unigram(UNIGRAM)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "bigram_model = Bigram(BIGRAM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "unigram_model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "bigram_model.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. Using the bigram model, continue the following sentence with the most probable word predicted by the model: “ I\n",
    "have a house in ... \"\"\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = nlp('I have a house in')\n",
    "predicted_word = bigram_model.predict(sentence)\n",
    "print(predicted_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 probability: -inf\n",
      "sentence 2 probability: -21.516756502901373\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 3. Using the bigram model:\n",
    "(a) compute the probability of the following two sentences (for each sentence separately).\n",
    "(b) compute the perplexity of both the following two sentences (treating them as a single test set with 2 sentences).\n",
    "\n",
    "Brad Pitt was born in Oklahoma\n",
    "The actor was born in USA\n",
    "\"\"\"\n",
    "sentence1 = nlp('Brad Pitt was born in Oklahoma')\n",
    "sentence2 = nlp('The actor was born in USA')\n",
    "\n",
    "sentence1_prob = bigram_model.probability(sentence1)\n",
    "sentence2_prob = bigram_model.probability(sentence2)\n",
    "print(f'sentence 1 probability: {sentence1_prob}')\n",
    "print(f'sentence 2 probability: {sentence2_prob}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 ans 2 perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "perplexity = bigram_model.perplexity([sentence1 , sentence2])\n",
    "print(f'sentence 1 ans 2 perplexity: {perplexity}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 interpolated probability: -16.742848003834535\n",
      "sentence 2 interpolated probability: -26.45504299020583\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Now we use linear interpolation smoothing between the bigram model and unigram model with λbigram = 2/3 and\n",
    "λunigram = 1/3, using the same training data. Given this new model, compute the probability and the perplexity of the\n",
    "same sentences such as in the previous question. \"\"\"\n",
    "interpolation = LinearInterpolation(unigram_model , bigram_model , 1/3 , 2/3)\n",
    "sentence1_inter_prob = interpolation.probability(sentence1)\n",
    "sentence2_inter_prob = interpolation.probability(sentence2)\n",
    "print(f'sentence 1 interpolated probability: {sentence1_inter_prob}')\n",
    "print(f'sentence 2 interpolated probability: {sentence2_inter_prob}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 ans 2 interpolated perplexity: 36.591802850992856\n"
     ]
    }
   ],
   "source": [
    "perplexity = interpolation.perplexity([sentence1 , sentence2])\n",
    "print(f'sentence 1 ans 2 interpolated perplexity: {perplexity}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}