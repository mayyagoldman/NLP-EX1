{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "PROB = 0\n",
    "WORD = 1\n",
    "UNIGRAM = 1\n",
    "BIGRAM = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.nlp = None\n",
    "        self.unigram_data = {}  # {w:count}\n",
    "        self.bigram_data = {}  # {w_prev: {w:count}}\n",
    "        self.M = 0\n",
    "        self.m = 0\n",
    "        self.unigram_data_size = 0\n",
    "        self.bigram_data_size = {}\n",
    "\n",
    "    def add_to_unigram(self, w):\n",
    "        self.unigram_data_size += 1\n",
    "        if w in self.unigram_data:\n",
    "            self.unigram_data[w] += 1\n",
    "        else:\n",
    "            self.unigram_data[w] = 1\n",
    "\n",
    "    def add_to_bigram(self, w, w_prev):\n",
    "        if w_prev not in self.bigram_data:\n",
    "            self.bigram_data[w_prev] = {}\n",
    "            self.bigram_data_size[w_prev] = 0\n",
    "        self.bigram_data_size[w_prev] += 1\n",
    "        w_prev_dict = self.bigram_data[w_prev]\n",
    "        if w in w_prev_dict:\n",
    "            w_prev_dict[w] += 1\n",
    "        else:\n",
    "            w_prev_dict[w] = 1\n",
    "\n",
    "    def load_data(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n",
    "        for text in dataset['text']:\n",
    "            self.m +=1\n",
    "            doc = self.nlp(text)\n",
    "            w_prev = 'START'\n",
    "            for w in doc:\n",
    "                if w.is_alpha:\n",
    "                    self.M +=1\n",
    "                    self.add_to_unigram(w.lemma_)\n",
    "                    self.add_to_bigram(w.lemma_, w_prev)\n",
    "                    w_prev = w.lemma_  # todo: check first word in extreme cases is indeed START\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "\n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        self.M = corpus.M #total num of tokens\n",
    "        self.m = corpus.m #total num of sentences\n",
    "        if n == 1:\n",
    "            self.corpus = corpus.unigram_data  # {w: count / N}\n",
    "            print(self.corpus)\n",
    "            self.max_prob = - np.inf\n",
    "            self.max_prob_word = None\n",
    "            self.total_count = corpus.unigram_data_size\n",
    "        elif n == 2:\n",
    "            self.corpus = corpus.bigram_data\n",
    "            self.max_probs = {}\n",
    "            self.total_counts = corpus.bigram_data_size\n",
    "\n",
    "    def train_unigram(self):\n",
    "        for w_i in self.corpus.keys():\n",
    "            count = self.corpus[w_i]\n",
    "            prob_w_i = np.log(count / self.total_count)\n",
    "            self.corpus[w_i] = prob_w_i\n",
    "            if prob_w_i > self.max_prob:\n",
    "                self.max_prob = prob_w_i\n",
    "                self.max_prob_word = w_i\n",
    "\n",
    "    def train_bigram(self):\n",
    "        for w_i  in self.corpus.keys():\n",
    "            proceeding_w_j = self.corpus[w_i]\n",
    "            self.max_probs[w_i] = [- np.inf, None]\n",
    "            for w_j in proceeding_w_j.keys():\n",
    "                count = proceeding_w_j[w_j]\n",
    "                prob_w_j = np.log(count / self.total_counts[w_i])\n",
    "                proceeding_w_j[w_j] = prob_w_j\n",
    "                if prob_w_j > self.max_probs[w_i][0]:\n",
    "                    self.max_probs[w_i][PROB] = prob_w_j\n",
    "                    self.max_probs[w_i][WORD] = w_j\n",
    "\n",
    "    def train(self):\n",
    "        if self.n == 1:\n",
    "            self.train_unigram()\n",
    "        elif self.n == 2:\n",
    "            self.train_bigram()\n",
    "\n",
    "    def _predict_unigram(self, sentence):\n",
    "        return self.max_prob_word\n",
    "\n",
    "    def _predict_bigram(self, sentence):\n",
    "        last_word = sentence[-1]\n",
    "        if last_word not in self.corpus:\n",
    "            return \"STOP\"\n",
    "        return self.max_probs[last_word][WORD]\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        if self.n == 1:\n",
    "            return self._predict_unigram(sentence)\n",
    "        elif self.n == 2:\n",
    "            return self._predict_bigram(sentence)\n",
    "\n",
    "    def _probability_unigram(self, sentence):\n",
    "        prob = 0\n",
    "        for w in sentence:\n",
    "            if w not in self.corpus:\n",
    "                return 0\n",
    "            prob += self.corpus[w]\n",
    "        return prob\n",
    "\n",
    "    def _probability_bigram(self, sentence):\n",
    "        if sentence[0] not in self.corpus['START']:\n",
    "            return 0\n",
    "        prob = self.corpus['START'][sentence[0]]\n",
    "        for i in range(2, len(sentence)):\n",
    "            w_prev = sentence[i - 1]\n",
    "            w_i = sentence[i]\n",
    "            if w_prev not in self.corpus:\n",
    "                return 0\n",
    "            if w_i not in self.corpus[w_prev]:\n",
    "                return 0\n",
    "            prob += self.corpus[w_prev][w_i]\n",
    "        return prob\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        if self.n == 1:\n",
    "            return self._probability_unigram(sentence)\n",
    "        elif self.n == 2:\n",
    "            return self._probability_bigram(sentence)\n",
    "\n",
    "    def perplexity(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "class LinearInterpolation:\n",
    "    def __init__(self, unigram, bigram):\n",
    "        self.unigram_model = unigram\n",
    "        self.unigram_model = bigram\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        pass\n",
    "\n",
    "    def perplexity(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/mayagoldman/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126 (last modified on Fri Nov 11 17:05:36 2022) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset wikitext (/Users/mayagoldman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(corpus.unigram_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"1. Train maximum-likelihood unigram and bigram language models based on the above training data.\"\"\"\n",
    "\n",
    "unigram_model = NGramModel(corpus, UNIGRAM)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigram_model = NGramModel(corpus, BIGRAM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_model.train_unigram()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigram_model.train_bigram()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"2. Using the bigram model, continue the following sentence with the most probable word predicted by the model: â€œ I\n",
    "have a house in ... \"\"\"\n",
    "sentence = corpus.nlp('I have a house in')\n",
    "predicted_word = bigram_model.predict(sentence)\n",
    "print(predicted_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" 3. Using the bigram model:\n",
    "(a) compute the probability of the following two sentences (for each sentence separately).\n",
    "(b) compute the perplexity of both the following two sentences (treating them as a single test set with 2 sentences).\n",
    "\n",
    "Brad Pitt was born in Oklahoma\n",
    "The actor was born in USA\n",
    "\"\"\"\n",
    "sentence1 = corpus.nlp('Brad Pitt was born in Oklahoma')\n",
    "sentence2 = corpus.nlp('The actor was born in USA')\n",
    "\n",
    "sentence1_prob = unigram_model.probability(sentence1)\n",
    "sentence2_prob = unigram_model.probability(sentence2)\n",
    "print(f'sentence 1 probability:{sentence1_prob}')\n",
    "print(f'sentence 2 probability:{sentence2_prob}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}