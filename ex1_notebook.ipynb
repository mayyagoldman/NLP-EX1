{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "PROB = 0\n",
    "WORD = 1\n",
    "UNIGRAM = 1\n",
    "BIGRAM = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    unigram_data = {}  # {w:count}\n",
    "    bigram_data = {}  # {w_prev: {w:count}}\n",
    "    M = 0       # Total number of tokens (words) in the test data\n",
    "    m = 0       # The number of sentences in the test data\n",
    "    unigram_data_size = 0\n",
    "    bigram_data_size = {}\n",
    "\n",
    "    def add_to_unigram(self, w):\n",
    "        self.unigram_data_size += 1\n",
    "        if w in self.unigram_data:\n",
    "            self.unigram_data[w] += 1\n",
    "        else:\n",
    "            self.unigram_data[w] = 1\n",
    "\n",
    "    def add_to_bigram(self, w_next, w_prev):\n",
    "        if w_prev not in self.bigram_data:\n",
    "            self.bigram_data[w_prev] = {}\n",
    "            self.bigram_data_size[w_prev] = 0\n",
    "        self.bigram_data_size[w_prev] += 1\n",
    "        w_prev_dict = self.bigram_data[w_prev]\n",
    "        if w_next in w_prev_dict:\n",
    "            w_prev_dict[w_next] += 1\n",
    "        else:\n",
    "            w_prev_dict[w_next] = 1\n",
    "\n",
    "    def load_data(self):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n",
    "        for text in dataset['text']:\n",
    "            self.m +=1\n",
    "            doc = nlp(text)\n",
    "            w_prev = 'START'\n",
    "            for w in doc:\n",
    "                if w.is_alpha:\n",
    "                    self.M +=1\n",
    "                    self.add_to_unigram(w.lemma_)\n",
    "                    self.add_to_bigram(w.lemma_, w_prev)\n",
    "                    w_prev = w.lemma_  # todo: check first word in extreme cases is indeed START\n",
    "        print(self.bigram_data['a'].values())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "\n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        self.M = corpus.M #total num of tokens\n",
    "        self.m = corpus.m #total num of sentences\n",
    "        if n == 1:\n",
    "            self.corpus = corpus.unigram_data  # {w: count / N}\n",
    "            print(self.corpus)\n",
    "            self.max_prob = - np.inf\n",
    "            self.max_prob_word = None\n",
    "            self.total_count = corpus.unigram_data_size\n",
    "        elif n == 2:\n",
    "            self.corpus = corpus.bigram_data\n",
    "            self.max_probs = {}\n",
    "            self.total_counts = corpus.bigram_data_size\n",
    "\n",
    "    def train_unigram(self):\n",
    "        for w_i in self.corpus.keys():\n",
    "            count = self.corpus[w_i]\n",
    "            prob_w_i = np.log(count / self.total_count)\n",
    "            self.corpus[w_i] = prob_w_i\n",
    "            if prob_w_i > self.max_prob:\n",
    "                self.max_prob = prob_w_i\n",
    "                self.max_prob_word = w_i\n",
    "\n",
    "    def train_bigram(self):\n",
    "        for w_i  in self.corpus.keys():\n",
    "            proceeding_w_j = self.corpus[w_i]\n",
    "            self.max_probs[w_i] = [- np.inf, None]\n",
    "            for w_j in proceeding_w_j.keys():\n",
    "                count = proceeding_w_j[w_j]\n",
    "                prob_w_j = np.log(count / self.total_counts[w_i])\n",
    "                proceeding_w_j[w_j] = prob_w_j\n",
    "                if prob_w_j > self.max_probs[w_i][0]:\n",
    "                    self.max_probs[w_i][PROB] = prob_w_j\n",
    "                    self.max_probs[w_i][WORD] = w_j\n",
    "\n",
    "    def train(self):\n",
    "        if self.n == 1:\n",
    "            self.train_unigram()\n",
    "        elif self.n == 2:\n",
    "            self.train_bigram()\n",
    "\n",
    "    def _predict_unigram(self, sentence):\n",
    "        return self.max_prob_word\n",
    "\n",
    "    def _predict_bigram(self, sentence):\n",
    "        last_word = sentence.split(\" \")[-1]\n",
    "        if last_word not in self.corpus:\n",
    "            return \"STOP\"\n",
    "        return self.max_probs[last_word][WORD]\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        if self.n == 1:\n",
    "            return self._predict_unigram(sentence)\n",
    "        elif self.n == 2:\n",
    "            return self._predict_bigram(sentence)\n",
    "\n",
    "    def _probability_unigram(self, sentence):\n",
    "        prob = 0\n",
    "        sentence = sentence.split(\" \")\n",
    "        for w in sentence:\n",
    "            if w not in self.corpus:\n",
    "                return 0\n",
    "            prob += self.corpus[w]\n",
    "        return prob\n",
    "\n",
    "    def _probability_bigram(self, sentence):\n",
    "        sentence = \"START \" + sentence\n",
    "        sentence = sentence.split(\" \")\n",
    "        prob = 0\n",
    "        for i in range(1, len(sentence)):\n",
    "            w_prev = sentence[i - 1]\n",
    "            w_i = sentence[i]\n",
    "            if w_prev not in self.corpus:\n",
    "                return 0\n",
    "            if w_i not in self.corpus[w_prev]:\n",
    "                return 0\n",
    "            prob += self.corpus[w_prev][w_i]\n",
    "        return prob\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        if self.n == 1:\n",
    "            return self._probability_unigram(sentence)\n",
    "        elif self.n == 2:\n",
    "            return self._probability_bigram(sentence)\n",
    "\n",
    "    def perplexity(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "class LinearInterpolation:\n",
    "    def __init__(self, unigram, bigram):\n",
    "        self.unigram_model = unigram\n",
    "        self.unigram_model = bigram\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        pass\n",
    "\n",
    "    def perplexity(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/mayagoldman/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [125]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m corpus \u001B[38;5;241m=\u001B[39m Corpus()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mcorpus\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [122]\u001B[0m, in \u001B[0;36mCorpus.load_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 32\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     w_prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSTART\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m doc:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/language.py:1026\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1024\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1026\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1028\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(corpus.unigram_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"1. Train maximum-likelihood unigram and bigram language models based on the above training data.\"\"\"\n",
    "\n",
    "unigram_model = NGramModel(corpus, UNIGRAM)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigram_model = NGramModel(corpus, BIGRAM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_model.train_unigram()\n",
    "print(unigram_model.corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigram_model.train_bigram()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"2. Using the bigram model, continue the following sentence with the most probable word predicted by the model: “ I\n",
    "have a house in ... \"\"\"\n",
    "sentence = 'I have a house in'\n",
    "predicted_word = bigram_model.predict(sentence)\n",
    "print(predicted_word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" 3. Using the bigram model:\n",
    "(a) compute the probability of the following two sentences (for each sentence separately).\n",
    "(b) compute the perplexity of both the following two sentences (treating them as a single test set with 2 sentences).\n",
    "\n",
    "Brad Pitt was born in Oklahoma\n",
    "The actor was born in USA\n",
    "\"\"\"\n",
    "sentence1 = 'Brad Pitt was born in Oklahoma'\n",
    "sentence2 = 'The actor was born in USA'\n",
    "\n",
    "sentence1_prob = unigram_model.probability(sentence1)\n",
    "sentence2_prob = unigram_model.probability(sentence2)\n",
    "print(f'sentence 1 probability:{sentence1_prob}')\n",
    "print(f'sentence 2 probability:{sentence2_prob}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}